# This is a basic workflow to help you get started with Actions

name: CI

# Controls when the workflow will run
on:
  # Triggers the workflow on push or pull request events but only for the "main" branch
  push:
    branches: [ "main" ]
  pull_request:
    branches: [ "main" ]

  # Allows you to run this workflow manually from the Actions tab
  workflow_dispatch:

# A workflow run is made up of one or more jobs that can run sequentially or in parallel
jobs:
  # This workflow contains a single job called "build"
  build:
    # The type of runner that the job will run on
    runs-on: ubuntu-latest

    # Steps represent a sequence of tasks that will be executed as part of the job
    steps:
      # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it
      - uses: actions/checkout@v4

      # Runs a single command using the runners shell
      - name: Run a one-line script
        run: echo Hello, world dumbass!

      # Runs a set of commands using the runners shell
      - name: Run a multi-line script
        run: |
          pwd
          ls -ltra
          # -e exits if any of the lines fails
          # -x prints each command that is going to be executed with a little plus
          set -ex

          # SET THE FOLLOWING VARIABLES
          REGISTRY=bigsilver07
          IMAGE=my-mkdocs-site
          TAG=1.0

          docker build -t $REGISTRY/$IMAGE:$TAG .

          set -ex

          docker run --rm -p 8000:8000 -v ${PWD}:/app --name my-docs -d bigsilver07/my-mkdocs-site:1.0 

      - name: Push to DockerHub
        uses: docker/build-push-action@v1
        with:
          username: ${{secrets.DOCKER_USERNAME}}
          password: ${{secrets.DOCKER_PASSWORD}}
          repository: bigsilver07/my-mkdocs-site
          tags: latest, ${{ github.run_number }}
          
      - name: Validate Website Links
        run: |
          # Define the base URL for the site
          BASE_URL="http://localhost:8000"
          LINKS_FILE="extracted_links.txt"
  
          # Use wget to crawl the site and extract links
          wget --spider --recursive --level=3 --output-file=wget.log "$BASE_URL"
  
          # Extract URLs from the wget log
          grep -Eo 'http[s]?://[^ ]+' wget.log | sort -u > "$LINKS_FILE"
  
          echo "Validating links found on the site..."
  
          # Initialize a variable to track overall status
          ALL_VALID=true
  
          # Check each link using curl
          while IFS= read -r link; do
            echo -n "Checking $link... "
            status_code=$(curl -o /dev/null -s -w "%{http_code}" "$link")
  
            if [[ "$status_code" =~ ^[23] ]]; then
              echo "OK ($status_code)"
            else
              echo "FAILED ($status_code)"
              ALL_VALID=false
            fi
          done < "$LINKS_FILE"
  
          # Fail the workflow if any link validation fails
          if [[ "$ALL_VALID" = false ]]; then
            echo "Some links failed validation."
            exit 1
          else
            echo "All links are valid!"
          fi
  

  
  
          

  

      




